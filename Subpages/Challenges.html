<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Challenges & Solutions</title>
	<link rel="stylesheet" type='text/css' href="Challenges.css">
</head>
<body>
	<header class="header">
		<div class = "header-text">
			<h1><b>Challenges & Solutions<br>for AI in Mental Health Services</b></h1>
      		<p>Discover the challenges AI faces in mental health, like emotional recognition, long-term tracking, and personalized treatment. Explore solutions such as multimodal systems and better data privacy to improve accuracy and care.</p>
      	</div>

      	<img class="top-right-icon" src="Challenges/challenge.png" alt="Challenges Icon">
	</header>

	<div class="container">
		<button id="prevBtn" class="nav-btn">&lt;</button>
	<div class="card active">
		<div class="cha">
			<img class="cha-icon" src="Challenges/cha1-icon.png" alt="Challenges Icon">
			
			<div class="cha-right-text">
				<h2><b>Limitations of Emotional Understanding</b></h2>
				<p><b>Challenge: </b>AI still faces challenges in processing subtle or complex emotional changes, especially when interpreting non-verbal cues such as body language and tone of voice. Moreover, emotional recognition often depends on pre-labeled datasets, which may not capture the full diversity of human emotions, affecting recognition accuracy.</p>
				<p><b>Solution: </b>Existing multimodal AI systems combine speech, text, and image analysis to enhance the accuracy of emotional recognition. Additionally, emotional intelligence algorithms are integrated, enabling the system to detect users' emotional needs more effectively and respond appropriately.</p>
			</div>
		</div>
	</div>

	<div class="card hidden">
		<div class="cha">
			<div class="cha-left-text">
				<h2><b>Lack of Long-Term Emotional Tracking</b></h2>
				<p><b>Challenge: </b>Most AI mental health tools focus on providing immediate feedback and lack long-term monitoring and comprehensive analysis of emotional fluctuations, making it difficult to detect subtle emotional shifts and unable to provide timely, effective psychological support when users' emotional states change.</p>
				<p><b>Solution: </b>Some AI systems have started incorporating long-term emotional tracking by utilizing mood logs, wearable devices, and physiological data to monitor emotional changes over time. These systems analyze patterns in users' emotional states and provide suggestions or interventions to help with self-regulation. However, current solutions still face limitations in terms of accuracy and responsiveness, and further advancements in technology are needed to improve long-term emotional support.</p>
			</div>

			<img class="cha-icon" src="Challenges/cha2-icon.png" alt="Challenges Icon">
		</div>
	</div>

	<div class="card hidden">
		<div class="cha">
			<img class="cha-icon" src="Challenges/cha3-icon.png" alt="Challenges Icon">
			
			<div class="cha-right-text">
				<h2><b>Limitations of Personalized Treatment Plans</b></h2>
				<p><b>Challenge: </b>Most AI systems currently face challenges in fully accounting for each user's unique background, past treatment responses, and even genetic predispositions. As a result, treatment plans may fail to adequately meet patients' individual needs, limiting the treatment's overall effectiveness.</p>
				<p><b>Solution: </b>To address this challenge, some AI platforms improve treatment personalization by analyzing emotional data, recognizing behavior patterns, and incorporating user feedback. Machine learning algorithms dynamically adjust intervention strategies based on emotional shifts, daily routines, and past treatment outcomes. Additionally, AI is used in collaboration with psychologists to analyze data and monitor emotions, allowing for more tailored and effective treatment recommendations.</p>
			</div>
		</div>
	</div>

	<div class="card hidden">
		<div class="cha4">
			<div class="cha4-text">
				<div class="cha-left-text">
					<h2><b>Data Privacy and Security Issues</b></h2>
					<p><b>Challenge: </b>AI mental health counseling systems process a large amount of sensitive personal data, which, if not properly secured, poses significant risks of data breaches. Additionally, finding a balance between gathering enough data for accurate recommendations and protecting user privacy, while ensuring compliance with relevant data privacy laws and regulations, remains a crucial challenge that needs to be addressed.</p>
					<p><b>Solution: </b>Most AI mental health platforms implement end-to-end encryption and data anonymization to protect user privacy. These platforms also adhere to international privacy regulations, such as GDPR and HIPAA, ensuring that data storage and processing meet privacy standards. Furthermore, users are granted the right to delete or withdraw their data, which helps reduce privacy risks and enhances user trust.</p>
				</div>

				<img class="cha-icon" src="Challenges/cha4-icon.png" alt="Challenges Icon">
			</div>

			<div class="regulations">
				<b>International Data Privacy Regulations:</b>
				<ul>
					<li><b>EU-GDPR(General Data Protection Regulation)</b></li>
					<p>The GDPR imposes strict requirements on AI-driven mental health platforms to obtain explicit user consent for processing sensitive data, such as emotional and mental health information. It ensures users retain control over their data, including access, modification, and deletion rights. The regulation emphasizes transparency, and data minimization, and requires encryption for cross-border data transfers, ensuring data security and protecting user privacy.</p>
					<li><b>USA-HIPAA(Health Insurance Portability and Accountability Act)</b></li>
					<p>HIPAA primarily safeguards the privacy and security of medical data, requiring AI platforms to use encryption technology to protect mental health data and implement access control mechanisms to prevent unauthorized access. Additionally, in the event of a data breach, HIPAA mandates that platforms promptly notify users and take corrective actions to ensure the security of user data.</p>
					<li><b>Taiwan's Data Privacy Regulations</b></li>
					<p>Taiwan currently does not have specific laws for AI-driven mental health tools, but there are personal data protection laws and AI development guidelines that provide a legal framework. More specific regulations are expected to be introduced in the future to ensure data security.</p>
					<ul>
						<li><b>Personal Data Protection Act:</b></li>
						<p>This law requires AI platforms to obtain explicit consent from users when collecting and processing their data, and to implement appropriate security measures to prevent data breaches or misuse.</p>
						<li><b>Draft of the Artificial Intelligence Basic Act:</b></li>
						<p>In July 2024, the National Science and Technology Council released a draft of the <b>Artificial Intelligence Basic Act</b>, proposing seven core principles for AI development. It emphasizes that AI systems should respect human autonomy and fundamental rights, ensuring privacy protection during data collection and usage to mitigate the risk of data leakage.</p>
					</ul>
				</ul>
			</div>
		</div>
	</div>

	<div class="card hidden">
		<div class="cha5">
			<h2><b>Liability and Ethical Challenges</b></h2>
			<p><b>Challenge: </b>As AI becomes more widely applied in mental health, issues of responsibility become complex. When AI systems provide incorrect advice or fail to identify warning signs in users, it can worsen mental health issues. However, AI is not a legal entity, making it difficult to determine whether developers, platform providers, or users should bear responsibility.</p>
			<p><b>Solution: </b>Most platforms position AI as a supportive tool in their terms of service, advising users to seek professional help in critical situations. Crisis management mechanisms are integrated to direct users to emergency contacts when danger signals are detected. Additionally, platforms conduct regular algorithm reviews to enhance the safety and reliability of the system.</p>

			<div class="example">
				<b>Examples: </b>
				<ul>
					<div class="example-app">
						<div class="example-text">
							<li><b>Woebot</b></li>
							<p>Woebot is clearly positioned as a supportive tool for mental health, using a crisis detection system to quickly identify potentially harmful language and immediately direct users to seek professional help. The platform also regularly updates its algorithms to improve the safety and reliability of its services.</p>
						</div>

						<img class="example-icon" src="Challenges/woebot-icon.png" alt="Challenges Icon">
					</div>
					
					<div class="example-app">
						<div class="example-text">
							<li><b>Ginger</b></li>
							<p>Ginger is positioned as an auxiliary tool, combining crisis management with continuous data monitoring. When abnormal data is detected, it immediately notifies the assigned coach for intervention, and it regularly reviews its algorithms to maintain the professionalism and accuracy of the service.</p>
						</div>

						<img class="example-icon" src="Challenges/ginger-icon.jpeg" alt="Challenges Icon">
					</div>
				</ul>
			</div>
		</div>
	</div>
	<button id="nextBtn" class="nav-btn">&gt;</button>
	</div>

	<footer class="footer"></footer>

	<script src="Challenges.js"></script>
</body>
</html>
